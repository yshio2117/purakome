# coding: UTF-8

#from models import Tweetdata
import classifier_sub
import numpy as np
import csv
import os, sys, django
#sys.path.append(os.path.join(os.path.dirname(os.path.abspath(__file__)), 'dkango_app'))
sys.path.append(os.getcwd())
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'django_app.settings')  # DJANGO_SETTINGS_MODULEã«ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®settings.pyã®ã‚’æŒ‡å®šã—ã¾ã™ã€‚
django.setup()
from sentiment.models import Tweetdata2,Summarys,Statistics1week,Official_names,Keywords,Prefectures # åˆ©ç”¨ã—ãŸã„ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã™ã€‚
from django.utils import timezone

# æ—¥ä»˜ã®è­¦å‘Šã‚’ç„¡è¦–
import warnings
warnings.filterwarnings('ignore')
from django.db.models import Q
# idé‡è¤‡å–å¾—ã‚¨ãƒ©ãƒ¼
from django.db.utils import IntegrityError
import re
import tokenizer
import datetime
import MeCab
import itertools
import neologdn,unicodedata
import math
import pprint
import random

#wordcloud
from wordcloud import WordCloud
from PIL import Image

from takesummary_forsite import take_summary
from twython import Twython
import gspread
CONSUMER_KEY = os.environ.get('CONSUMER_KEY')
CONSUMER_SECRET =os.environ.get('CONSUMER_SECRET') 
ACCESS_TOKEN = os.environ.get('ACCESS_TOKEN')
ACCESS_TOKEN_SECRET = os.environ.get('ACCESS_TOKEN_SECRET')


def keywords_update():
    
    print("accessing the spreadsheet..",end='')
    gc = gspread.service_account()
    #ãƒ•ã‚¡ã‚¤ãƒ«ã‚ªãƒ¼ãƒ—ãƒ³
    sh = gc.open_by_key('1b-89Ynrpek39vny4sPfkaYqNp-WYLZJ6txAjaEAld9I') # anime_keywords
    worksheet = sh.worksheet('index_summary')
 
    tmp = worksheet.col_values(1)
    tmp.pop(0)
    indexes = [int(i) for i in tmp]

    
    # keywordã¯ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§è¤‡æ•°ã‚ã‚Š
    keywords = [i.split(',') for i in worksheet.col_values(5)]
    keywords.pop(0)
    

    if len(indexes) != len(keywords):
        return 'number of indexes is different.'
    
    pairs = [(i,k) for i,k in zip(indexes,keywords) if k != ['']] # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç©ºæ¬„(æ¬ ç•ª)ã¯é™¤å¤–

    print("deleting DB..")
    
    Keywords.objects.all().delete()

        
    print("done.")
    
    print('bulk update..')                                              
    update_list = []
    for pair in pairs:
        for key in pair[1]:
            update_list.append(Keywords(official_name_id = pair[0],
                                              keyword = key
                                              ))

        
    Keywords.objects.bulk_create(update_list)


    print("done.")
    print("done.")

    return 'success.'
    
    
def official_names_update():
    
    print("accessing the spreadsheet..",end='')
    gc = gspread.service_account()
    #ãƒ•ã‚¡ã‚¤ãƒ«ã‚ªãƒ¼ãƒ—ãƒ³
    sh = gc.open_by_key('1b-89Ynrpek39vny4sPfkaYqNp-WYLZJ6txAjaEAld9I') # anime_keywords
    worksheet = sh.worksheet('index_summary')

    tmp = worksheet.col_values(1)
    tmp.pop(0)
    indexes = [int(i) for i in tmp]


    tmp = worksheet.col_values(2)
    tmp.pop(0)
    titles = [i for i in tmp]


    official_names = [i for i in worksheet.col_values(3)]
    official_names.pop(0)


    title_names = [i for i in worksheet.col_values(4)]
    title_names.pop(0)    

    
    if (len(indexes) != len(titles) or len(titles) != len(official_names) or len(titles) != len(title_names)):
        return 'number of indexes is different.'


        
        
    print("updating Official_names..")
    
    #Official_names.objects.all().delete()
    for index,title,official_name,title_name in zip(indexes,titles,official_names,title_names):
        obj, created = Official_names.objects.update_or_create(
            index = index,

            defaults={
                'title' : True if title == 'TRUE' else False,
                'official_name' : official_name,
                'title_name' : title_name
            }
        )


    print("done.")

    return 'success.'
            


def dataupdate(days=1):
    
    
    dt_now = datetime.datetime.now()
    e_date = datetime.datetime(dt_now.year,
                              dt_now.month, 
                              dt_now.day, 0, 0, 0, 0,tzinfo=datetime.timezone.utc) - datetime.timedelta(hours=9)
    s_date = e_date - datetime.timedelta(days=days) 
    print('date:{0}-{1}'.format(s_date,e_date))

    # å…¨ã‚­ãƒ£ãƒ©åå–å¾—
    keywords = list(Keywords.objects.all().values())
    print('keywords:',keywords)
    print("done.")

    print("accesing DB..")
    #DBã‹ã‚‰æœ¬æ–‡å–å¾—
    ## æœ€å¤§14æ—¥ç¨‹åº¦ç›®å®‰
    data = Tweetdata2.objects.filter(t_date__range=[s_date,
                                                    e_date]
                                       ).only('id',
                                            'content',
                                            's_class',
                                            'wakachi',
                                            'spam',
                                            'title1',
                                            'title2',
                                            'title3',
                                            'character1',
                                            'character2',
                                            'character3',
                                            'character4',
                                            'character5').order_by('id') # ä½•ã‚‰ã‹ã®ã‚­ãƒ¼ã§ä¸¦ã³æ›¿ãˆãªã„ã¨bulk updateãŒæ­£ã—ã„é †åºã§updateã§ããªã„ã‚‰ã—ã„
    
    
    print("done.")
    
    contents = list(data.values_list("content", flat=True))
    print('total:',len(contents))

    # char_dicåˆ¤å®šç”¨ contentsä½œæˆ(ã‚­ãƒ£ãƒ©åã‚’æ­£è¦åŒ–ã—ãªã„&neolognd.normalizeã‚’è¿½åŠ ã§ã™ã‚‹)
    ptn1 = re.compile(r'https://[^\s]+')
    contents_chk = [(ptn1.sub("",neologdn.normalize(unicodedata.normalize('NFKC',i)))).strip() for i in contents]
    if len(contents) != len(contents_chk):
        return "len(contents_chk) error!"
        
    print("done.")
    
    print("å½¢æ…‹ç´ è§£æ SPAMåˆ¤å®š Keywordæ•´ç†..")
    # wakachi
    tagger = MeCab.Tagger(r'-u C:\Users\yusuk\wikidump_20220429.dic')
    wakachis = [] # wordcloud
    spams = [] # spamåˆ¤å®š
    spam_l = ['åœ¨åº«','å®šä¾¡','ä¾¡æ ¼','å®Œå£²','å®Ÿæ–½ä¸­','é…ä¿¡ä¸­','é–‹å‚¬ä¸­','æ ªå¼ä¼šç¤¾','å—æ³¨','å£²ã‚Œç­‹','æ‰€æŒ','å¥½è©•','æ¥½å¤©å¸‚å ´','ãŠè²·ã„å¾—','è³ªå•ç®±','è­²æ¸¡','æ±‚','è­²','è­²ã‚Š','å£²','è²·','äº¤æ›','æä¾›','æ¤œè¨','è²©å£²','äºˆç´„è²©å£²','ç™ºå£²','æ–°ç™ºå£²','å…¥è·','å†å…¥è·','è²·å–','DM','é€æ–™'] # 20220116 è­²ã‚Š,äº¤æ›ã€€è¿½åŠ 
    title1 = []
    title2 = []
    title3 = []
    character1 = []
    character2 = []
    character3 = []
    character4 = []
    character5 = []
    
    p1 = re.compile(r'[@ï¼ ][a-zA-Z_0-9]+') # \wãŒã†ã¾ãã„ã‹ãªã„?
    p2 = re.compile(r'ç†Ÿå¥³|ãƒãƒƒã‚µãƒ¼ã‚¸|æ•°é‡é™å®š|æ™¯å“æƒ…å ±|ãƒ—ãƒ©ã‚¤ã‚ºæƒ…å ±|ã‚­ãƒ£ãƒ©è¨ºæ–­|åº—é ­|ç‰¹å…¸|ã‚³ã‚³ã‹ã‚‰ã©ã†ã|æ˜¯é|ãŠå£°ãŒã‘|ãŠå£°æ›ã‘|rakuten|æ¿€å®‰|ãŠè­²ã‚Š|å•†å“æƒ…å ±|å•†å“ç´¹ä»‹|æ¼«ç”»ç´¹ä»‹|ãŠå¾—ãª|ã—ããŠé¡˜ã„è‡´ã—ã¾ã™|ã—ããŠé¡˜ã„ã„ãŸã—ã¾ã™|shindanmaker')
    p3 = re.compile(r'\n')
    for content,content_chk in zip(contents,contents_chk):

# title,character no å–å¾—        

        title_nos = []
        character_nos = []
        
        for keyword in keywords:
            if keyword['keyword'] in p1.sub('',content_chk): # mentionå†…ã¯é™¤å¤–
                record = Official_names.objects.get(index=keyword['official_name_id'])
                if record.title == True:
                    title_nos.append(record.index)
                else:
                    character_nos.append(record.index)
                    # ã‚­ãƒ£ãƒ©ãŒå±ã™ã‚‹ã‚¢ãƒ‹ãƒ¡å
                    try:
                        title_nos.append(Official_names.objects.get(title_name=record.title_name,
                                                                title=True).index)
                    except:
                        print('record',record)
                        print('record.title_name',record.title_name)
                        return 'error'

        title_nos = list(set(title_nos))
        character_nos = list(set(character_nos))
  
        # title3,chara5ã«æº€ãŸãªã„å ´åˆã¯Noneã§åˆæœŸåŒ–
        rest = 3 - len(title_nos)
        for r in range(rest):
            title_nos.append(None)
        rest = 5 - len(character_nos)
        for r in range(rest):
            character_nos.append(None)   

        title1.append(title_nos[0])
        title2.append(title_nos[1])
        title3.append(title_nos[2])
        character1.append(character_nos[0])
        character2.append(character_nos[1])
        character3.append(character_nos[2])
        character4.append(character_nos[3])
        character5.append(character_nos[4])        
# ------------------        
            
        #node = tagger.parseToNode(tokenizer.cleansing_text(content))
        node = tagger.parseToNode(content_chk) # ã‚­ãƒ£ãƒ©åã‚’wordcloudã«å…¥ã‚ŒãŸã„ã®ã§tokenizer.cleaningã¯ä½¿ã‚ãªã„
        flag = False # spamãƒ•ãƒ©ã‚°
        if title1[-1] == None: # ã‚¢ãƒ‹ãƒ¡ã§ã¯ãªã„æŠ•ç¨¿ã®å ´åˆ
            flag = True
        if p2.search(content_chk.lower()) != None: #spamå˜èªãŒåŸæ–‡ã«å«ã¾ã‚Œã‚‹å ´åˆ
            flag = True
        if len(p3.findall(content_chk))>=8: # æ”¹è¡ŒãŒå¤šã™ãã‚‹å ´åˆã‚‚spam
            flag = True
        wakachi = []
        while node:
            features = node.feature.split(',')
            if features[0] != 'BOS/EOS':
                if flag == False and features[0] == 'åè©' and features[6].lower() in spam_l:
                    flag = True # æ–‡å†…ã«ä¸€ã¤ã§ã‚‚è©²å½“ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã‚Œã°spamæ‰±ã„
                
                if (features[0] == 'å½¢å®¹è©' and features[1] == 'è‡ªç«‹') or (features[0] == 'åè©' and features[1] == 'ä¸€èˆ¬') or features[1] == 'å½¢å®¹å‹•è©èªå¹¹':
                    token = features[6] if features[6] != '*' else node.surface
                    wakachi.append(token)
                
            node = node.next
        spams.append(flag)
    
        wakachis.append(' '.join(list(set(wakachi))))
        
    
    print("done.")
    print("contents:",len(contents))
    print('wakachi:',len(wakachis))
    print('spam:',len(spams))
    print('t1:',len(title1))
    print('t2:',len(title2))
    print('t3:',len(title3))
    print('c1:',len(character1))
    print('c2:',len(character2))
    print('c3:',len(character3))
    print('c4:',len(character4))
    print('c5:',len(character5))
    
    #è­˜åˆ¥å™¨ã¨è¾æ›¸èª­ã¿è¾¼ã¿
    bunruiki = classifier_sub.Myclassifier()
    vocab,vocab2,model = bunruiki.load_mlp()  
            

    print("data predicting by mlp bm25..")
    predictions = []
    start = 0
    l_size = len(contents)
    while True:
        end = start + 100000
        predictions.append(bunruiki.predict_mlp(contents[start:end],vocab,vocab2,model)) # np.str_
        start = end
    
        if start > l_size:
            print("100% done.")
            break
        print("{0}% done.".format(round(100*end/l_size)))
    predictions = list(itertools.chain.from_iterable(predictions))
    
    print(len(predictions))
    print("done.")


    
    print("DB update..")
    # DB update
    # salormoon é€šå¸¸æ›´æ–°ã¯240000æŠ•ç¨¿ã§2æ™‚é–“ã‹ã‹ã‚‹
    # batchå‡¦ç†ã€€åŒsalormoon 5åˆ†ç¨‹åº¦ã§çµ‚äº†
    update_list = []
    print("making update_list..")

    for prediction,wakachi,spam,t1,t2,t3,c1,c2,c3,c4,c5,obj in zip(predictions,wakachis,spams,title1,title2,title3,character1,character2,character3,character4,character5,data):
        
        obj.s_class = prediction
        obj.wakachi = wakachi
        if prediction == 2: # eã¯ã‚¹ãƒ‘ãƒ æ‰±ã„
            obj.spam = True
        else:
            obj.spam = spam
        
        obj.title1 = t1
        obj.title2 = t2
        obj.title3 = t3
        obj.character1 = c1
        obj.character2 = c2
        obj.character3 = c3
        obj.character4 = c4
        obj.character5 = c5
        update_list.append(obj)
    print('done.')
    print('bulk update..')
    # ã‚³ãƒ”ãƒ¼ã‚’å–å¾—ã—ãŸæ™‚ã¨åŒã˜é †åºã§ãªã„ã¨æ­£ã—ã„é †ç•ªã§bulk updateã§ããªã„
    data.bulk_update(update_list, fields=['s_class',
                                            'wakachi',
                                            'spam',
                                            'title1',
                                            'title2',
                                            'title3',
                                            'character1',
                                            'character2',
                                            'character3',
                                            'character4',
                                            'character5'],batch_size=1000)
    
    return "done."


    # httpå‰Šé™¤
HTTP = re.compile(r'http[^\s]+')
HASHTAG = re.compile(r'[#ï¼ƒ][^\s]+')
MENTION = re.compile(r'[@ï¼ ][a-zA-Z_0-9]+') # \wãŒã†ã¾ãã„ã‹ãªã„?
ETC = re.compile(r'â€¦+|ã€‚+|ã€+|\!+|\?+')
GREETING = re.compile(r'ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™|ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™|ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸ|ã“ã‚“ã«ã¡ã¯|ã“ã‚“ã«ã¡ã‚|ã“ã‚“ã°ã‚“ã¯|ã“ã‚“ã°ã‚“ã‚|ãŠç–²ã‚Œæ§˜ã§ã™')
EMOJI = re.compile(r'[ğŸ˜‰ğŸ˜ğŸ˜‚ğŸ˜ƒğŸ˜„ğŸ˜…ğŸ˜†ğŸ˜‡ğŸ˜ˆğŸ˜‰ğŸ˜ŠğŸ˜‹ğŸ˜ŒğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜‘ğŸ˜’ğŸ˜“ğŸ˜”ğŸ˜•ğŸ˜–ğŸ˜—ğŸ˜˜ğŸ˜™ğŸ˜šğŸ˜›ğŸ˜œğŸ˜ğŸ˜ğŸ˜ŸğŸ˜ ğŸ˜¡ğŸ˜¢ğŸ˜£ğŸ˜¤ğŸ˜¥ğŸ˜¦ğŸ˜§ğŸ˜¨ğŸ˜©ğŸ˜ªğŸ˜«ğŸ˜¬ğŸ˜­ğŸ˜®ğŸ˜¯ğŸ˜°ğŸ˜±ğŸ˜²ğŸ˜³ğŸ˜´ğŸ˜µğŸ˜¶ğŸ˜·ğŸ˜¸ğŸ˜¹ğŸ˜ºğŸ˜»ğŸ˜¼ğŸ˜½ğŸ˜¾ğŸ˜¿ğŸ™€ğŸ™ğŸ™‚ğŸ™ƒğŸ™„ğŸ¤ğŸ¤‘ğŸ¤’ğŸ¤“ğŸ¤£ğŸ¤”ğŸ¥ºğŸ™‡â€ğŸ™â˜”ï¸ğŸ™‹â€ğŸ’¦ğŸ’•]')
def cleaning(document, tagger):
    
    
    text = document[0] # (content, id)
    text = HTTP.sub('',text)
    text = HASHTAG.sub('',text)
    text = MENTION.sub('',text)
    text = GREETING.sub('',text)
    text = ETC.sub('ã€‚',text)
    text = EMOJI.sub('ã€‚',text)
    text = neologdn.normalize(text)
    text = text.strip()
    text = text.lower()

    node = tagger.parseToNode(text)

    w_classes = []
    while node:
        features = node.feature.split(',')
        if features[0] != 'BOS/EOS':
            w_classes.append((features[0], features[1]))
        node = node.next

    if ('åè©', 'ä¸€èˆ¬') in w_classes or ('åè©', 'å›ºæœ‰åè©') in w_classes:
        if (('å‹•è©', 'è‡ªç«‹') in w_classes or
            ('å½¢å®¹è©', 'è‡ªç«‹') in w_classes or
            ('åè©', 'å½¢å®¹å‹•è©èªå¹¹') in w_classes or
            ('åè©', 'ã‚µå¤‰æ¥ç¶š') in w_classes):
            
            return (text, document[1])

    return None


def statistics_update():
    """summaryä»¥å¤–ã®é …ç›®ã®æ›´æ–°"""
    

    #Statistics1week.objects.all().delete()
    
    # dateæ¤œç´¢ã§t_date__range=[s,e]ã®å ´åˆã¯s,eã¨ã‚‚ã«utcæ™‚é–“ã§æŒ‡å®šã—ãªã„ã¨æ•°ãŒåˆã‚ãªããªã‚‹
    ## t_date__date=d ã§1æ—¥æŒ‡å®šã®å ´åˆã¯è‡ªå‹•ã§utcæ™‚é–“ã«å¤‰æ›ã•ã‚Œã‚‹ã‚ˆã†ã€‚å•é¡Œãªãå–å¾—ã§ãã‚‹
    dt_now = datetime.datetime.now()
    e_date = datetime.datetime(dt_now.year,
                              dt_now.month, 
                              dt_now.day, 0, 0, 0, 0,tzinfo=datetime.timezone.utc) - datetime.timedelta(hours=9)
    s_date = e_date - datetime.timedelta(days=7)
    print('date:{0}-{1}'.format(s_date,e_date))  

    title_nos = list(Official_names.objects.filter(title=True,index__gte=1).values_list('index',flat=True))
    chara_nos = list(Official_names.objects.filter(title=False,index__gte=1).values_list('index',flat=True))
    print('titles:',title_nos)
    print('charas',chara_nos)

    
    #DBã‹ã‚‰æœ¬æ–‡å–å¾—
    ## æœ€å¤§14æ—¥ç¨‹åº¦ç›®å®‰

    # å…¨ã‚­ãƒ£ãƒ©å–å¾—ã™ã‚‹ã¨ï¼‘æ™‚é–“ä»¥ä¸Šã‹ã‹ã‚‹ãŸã‚çµã‚Šè¾¼ã¿å¿…è¦
    ## tableã«ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã ã‘ã§ãªãã‚¢ãƒ‹ãƒ¡ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚‚å¿…è¦(å…¨ã‚­ãƒ£ãƒ©ã‹ã‚‰sumãŒå‡ºæ¥ãªã„ãŸã‚)
    
    for chara_no in chara_nos:
        print('chara:{0}'.format(chara_no))

        keys = ['4_count_d1','3_count_d1','0_count_d1','1_count_d1','4_count_d2','3_count_d2','0_count_d2','1_count_d2','4_count_d3','3_count_d3','0_count_d3','1_count_d3','4_count_d4','3_count_d4','0_count_d4','1_count_d4','4_count_d5','3_count_d5','0_count_d5','1_count_d5','4_count_d6','3_count_d6','0_count_d6','1_count_d6','4_count_d7','3_count_d7','0_count_d7','1_count_d7']
        values = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]
        d = dict(zip(keys, values))   
          
        allobj = Tweetdata2.objects.filter(Q(spam=False),
                                            Q(s_class=0) | Q(s_class=1) | Q(s_class=3) | Q(s_class=4),
                                            Q(t_date__range=[s_date,e_date]),
                                            Q(character1=chara_no) | 
                                            Q(character2=chara_no) | 
                                            Q(character3=chara_no) | 
                                            Q(character4=chara_no) | 
                                            Q(character5=chara_no)).only('t_date','s_class')
        li = list(allobj.values_list('t_date','s_class')) # tupleã®ãƒªã‚¹ãƒˆã¨ã—ã¦å–ã‚Šå‡ºã™

        for k in range(1,8):
            search_date_from = e_date - datetime.timedelta(days=k)
            search_date_until = search_date_from + datetime.timedelta(days=1)
            for c in range(0,5):
                if c == 2:
                    continue
                d['{0}_count_d{1}'.format(c, k)] = len([l[0] for l in li 
                                                        if l[0] >= search_date_from
                                                        and l[0] <= search_date_until
                                                        and l[1] == c
                                                        ])
                
        print('d:',d)
        print('data saving..')
        obj, created = Statistics1week.objects.update_or_create(
                        official_name_id=chara_no,
                        defaults={'s_date':s_date,'e_date':e_date,'p2_count_d1':d['4_count_d1'],'p1_count_d1':d['3_count_d1'],'n1_count_d1':d['1_count_d1'],'n2_count_d1':d['0_count_d1'],'p2_count_d2':d['4_count_d2'],'p1_count_d2':d['3_count_d2'],'n1_count_d2':d['1_count_d2'],'n2_count_d2':d['0_count_d2'],'p2_count_d3':d['4_count_d3'],'p1_count_d3':d['3_count_d3'],'n1_count_d3':d['1_count_d3'],'n2_count_d3':d['0_count_d3'],'p2_count_d4':d['4_count_d4'],'p1_count_d4':d['3_count_d4'],'n1_count_d4':d['1_count_d4'],'n2_count_d4':d['0_count_d4'],'p2_count_d5':d['4_count_d5'],'p1_count_d5':d['3_count_d5'],'n1_count_d5':d['1_count_d5'],'n2_count_d5':d['0_count_d5'],'p2_count_d6':d['4_count_d6'],'p1_count_d6':d['3_count_d6'],'n1_count_d6':d['1_count_d6'],'n2_count_d6':d['0_count_d6'],'p2_count_d7':d['4_count_d7'],'p1_count_d7':d['3_count_d7'],'n1_count_d7':d['1_count_d7'],'n2_count_d7':d['0_count_d7']}
                        )
        if created == True:
            print("added new record.")
        else:
            print("updated record.")
    
    for title_no in title_nos:
        print('title:{0}'.format(title_no))        

        keys = ['4_count_d1','3_count_d1','0_count_d1','1_count_d1','4_count_d2','3_count_d2','0_count_d2','1_count_d2','4_count_d3','3_count_d3','0_count_d3','1_count_d3','4_count_d4','3_count_d4','0_count_d4','1_count_d4','4_count_d5','3_count_d5','0_count_d5','1_count_d5','4_count_d6','3_count_d6','0_count_d6','1_count_d6','4_count_d7','3_count_d7','0_count_d7','1_count_d7']
        values = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]
        d = dict(zip(keys, values))        
        
        allobj = Tweetdata2.objects.filter(Q(spam=False),
                                    Q(s_class=0) | Q(s_class=1) | Q(s_class=3) | Q(s_class=4),
                                    Q(t_date__range=[s_date,e_date]),
                                    Q(title1=title_no) | 
                                    Q(title2=title_no) | 
                                    Q(title3=title_no)).only('t_date','s_class')
        li = list(allobj.values_list('t_date','s_class')) # tupleã®ãƒªã‚¹ãƒˆã¨ã—ã¦å–ã‚Šå‡ºã™

        
        for k in range(1,8):
            search_date_from = e_date - datetime.timedelta(days=k)
            search_date_until = search_date_from + datetime.timedelta(days=1)

            for c in range(0,5):
                if c == 2:
                    continue
                #print("accesing DB:{0}..".format(char_no[1]))                
                d['{0}_count_d{1}'.format(c, k)] = len([l[0] for l in li 
                                                        if l[0] >= search_date_from
                                                        and l[0] <= search_date_until
                                                        and l[1] == c
                                                        ])
                #print('done')
        
        print(d)
        print('data saving..')
        obj, created = Statistics1week.objects.update_or_create(
                        official_name_id=title_no,
                        defaults={'s_date':s_date,'e_date':e_date,'p2_count_d1':d['4_count_d1'],'p1_count_d1':d['3_count_d1'],'n1_count_d1':d['1_count_d1'],'n2_count_d1':d['0_count_d1'],'p2_count_d2':d['4_count_d2'],'p1_count_d2':d['3_count_d2'],'n1_count_d2':d['1_count_d2'],'n2_count_d2':d['0_count_d2'],'p2_count_d3':d['4_count_d3'],'p1_count_d3':d['3_count_d3'],'n1_count_d3':d['1_count_d3'],'n2_count_d3':d['0_count_d3'],'p2_count_d4':d['4_count_d4'],'p1_count_d4':d['3_count_d4'],'n1_count_d4':d['1_count_d4'],'n2_count_d4':d['0_count_d4'],'p2_count_d5':d['4_count_d5'],'p1_count_d5':d['3_count_d5'],'n1_count_d5':d['1_count_d5'],'n2_count_d5':d['0_count_d5'],'p2_count_d6':d['4_count_d6'],'p1_count_d6':d['3_count_d6'],'n1_count_d6':d['1_count_d6'],'n2_count_d6':d['0_count_d6'],'p2_count_d7':d['4_count_d7'],'p1_count_d7':d['3_count_d7'],'n1_count_d7':d['1_count_d7'],'n2_count_d7':d['0_count_d7']}
                        )
        if created == True:
            print("added new record.")
        else:
            print("updated record.")
        
    print("done.")
    return 'Success.'


def clean_summary():
    ### Tweetdata2ã‹ã‚‰è¦ç´„ã‚¯ãƒªã‚¢ ###
    
    
    dt_now = datetime.datetime.now()
    e_date = datetime.datetime(dt_now.year,
                              dt_now.month, 
                              dt_now.day, 0, 0, 0, 0,tzinfo=datetime.timezone.utc) - datetime.timedelta(hours=9)
    s_date = e_date - datetime.timedelta(days=7)
    
    data = Tweetdata2.objects.filter(Q(spam=False),
                                     Q(t_date__range=[s_date,e_date]),
                                       summary_brand_id__gte=1,
                                       ).only('id','summary_brand_id').order_by('id') #
    print('len(data):',len(data))
    update_list = []
    for obj in data:
       
        obj.summary_brand_id = 0
        update_list.append(obj)
        
    print("deleting tweetdata2 summarys..")
    data.bulk_update(update_list,
                    fields=['summary_brand_id',
                            #'summary_text'
                            ],
                    batch_size=5000
                    ) 
    print("done.")
    

                                                           
    return 'done.'

    
def sumupdate():
    
    
    dt_now = datetime.datetime.now()
    e_date = datetime.datetime(dt_now.year,
                              dt_now.month, 
                              dt_now.day, 0, 0, 0, 0,tzinfo=datetime.timezone.utc) - datetime.timedelta(hours=9)
    s_date = e_date - datetime.timedelta(days=7)
    print('date:{0}-{1}'.format(s_date,e_date))  

    # å…¨ã‚­ãƒ£ãƒ©åå–å¾—
    keywords = list(Keywords.objects.all().values_list('keyword', flat=True))
    print('keywords:',keywords)
    print("done.")
    
    title_nos = list(Official_names.objects.filter(title=True,index__gte=1).values_list('index',flat=True))
    chara_nos = list(Official_names.objects.filter(title=False,index__gte=1).values_list('index',flat=True))
    print('titles:',title_nos)
    print('charas',chara_nos)


    for cnt in range(2):
        if cnt == 0:
            target_nos = title_nos
        else:
            target_nos = chara_nos
                
        for target_no in target_nos:
            print("accesing DB:{0}".format(target_no))
            #DBã‹ã‚‰æœ¬æ–‡å–å¾—
            if cnt == 0:                
                data = Tweetdata2.objects.filter(Q(spam=False),
                                                 Q(summary_brand_id=0),# ä»–ã®ã‚¢ãƒ‹ãƒ¡ã§è¦ç´„ã•ã‚Œã¦ã„ãªã„
                                                 Q(t_date__range=[s_date,e_date]),
                                                 Q(title1=target_no) | Q(title2=target_no) | Q(title3=target_no),
                                                 # title1ã®ã¿ï¼ˆä»–ã‚‚è¦‹ã‚‹ã¨è¤‡æ•°ã‚¢ãƒ‹ãƒ¡ã®è¦ç´„ãŒä¸€ç·’ã«ãªã£ã¦ã—ã¾ã†
                                                   ).only('id',
                                                          's_class',
                                                          'content',
                                                          't_id',
                                                          'u_id',
                                                          'summary_brand_id',
                                                          'summary_no'
                                                          ).order_by('id') # ä½•ã‚‰ã‹ã®ã‚­ãƒ¼ã§ä¸¦ã³æ›¿ãˆãªã„ã¨bulk updateãŒæ­£ã—ã„é †åºã§updateã§ããªã„ã‚‰ã—ã„
            #DBã‹ã‚‰æœ¬æ–‡å–å¾—
            if cnt == 1:                
                data = Tweetdata2.objects.filter(Q(spam=False),
                                                 Q(summary_brand_id=0),# ä»–ã®ã‚¢ãƒ‹ãƒ¡ã§è¦ç´„ã•ã‚Œã¦ã„ãªã„
                                                 Q(t_date__range=[s_date,e_date]),
                                                 Q(character1=target_no) | Q(character2=target_no) | Q(character3=target_no) | Q(character4=target_no) | Q(character5=target_no),
                                                 # title1ã®ã¿ï¼ˆä»–ã‚‚è¦‹ã‚‹ã¨è¤‡æ•°ã‚¢ãƒ‹ãƒ¡ã®è¦ç´„ãŒä¸€ç·’ã«ãªã£ã¦ã—ã¾ã†
                                                   ).only('id',
                                                          's_class',
                                                          'content',
                                                          't_id',
                                                          'u_id',
                                                          'summary_brand_id',
                                                          'summary_no'
                                                          ).order_by('id') # ä½•ã‚‰ã‹ã®ã‚­ãƒ¼ã§ä¸¦ã³æ›¿ãˆãªã„ã¨bulk updateãŒæ­£ã—ã„é †åºã§updateã§ããªã„ã‚‰ã—ã„
        
           
                                                      
            allobj = list(data.values())                                                  
            print("done. len:{0}".format(len(allobj)))
        
        # è¦ç´„å–å¾—2
            print("take summarys..")
            sim_texts_all = [] #posi, negaè¨ˆ
            for pn in range(2): # 0:posi, 1:nega
                if pn == 0:
                    #p1,p2åˆè¨ˆ
                    documents = [(obj['content'],obj['t_id'],obj['u_id']) for obj in allobj if obj['s_class'] in [3,4]]
                    documents = get_unique_list(documents,2) # u_idé‡è¤‡å‰Šé™¤
                    documents = [(doc[0],doc[1]) for doc in documents]
                if pn == 1:
                    #n1,n2åˆè¨ˆ
                    documents = [(obj['content'],obj['t_id'],obj['u_id']) for obj in allobj if obj['s_class'] in [0,1]]
                    documents = get_unique_list(documents,2) # u_idé‡è¤‡å‰Šé™¤
                    documents = [(doc[0],doc[1]) for doc in documents]    
                # å‰å‡¦ç†ï¼‹ä¸»èªãƒ»è¿°èªãŒå«ã¾ã‚Œã‚‹æŠ•ç¨¿ã«ã—ã¼ã‚‹
                tagger = MeCab.Tagger(r'-u C:\Users\yusuk\wikidump_20220429.dic')
                docs_cleaned = []
                for d in documents:
                    tmp = cleaning(d, tagger) # tupleã§è¿”å´ã•ã‚Œã‚‹
                    if tmp != None:
                        docs_cleaned.append(tmp)
                #documents = [cleaning(d, tagger) for d in documents] # å‰å‡¦ç†
                docs_cleaned = get_unique_list(docs_cleaned,0)
                
                numd = len(docs_cleaned)
                print('docs_cleaned len:',numd)
                
                if numd < 30:
                    print('No enough data.')
                    sim_texts = [None for i in range(20)]

                else:
                    docs_cleaned = docs_cleaned[:10000] #3ä¸‡ç¨‹åº¦ã§ãƒ¡ãƒ¢ãƒªçˆ†ç™ºã™ã‚‹
                    sim_texts = summary_cos.get_sim(docs_cleaned, cos = 0.45, dup_degree = 0.2, t_id=True)
    
                    topics = summary_cos.get_topics(sim_texts, cos= 0.6, dup_degree = 0.2)
    
                    for i,topic in enumerate(topics):
                        sim_texts[i] = [topic] + sim_texts[i]
                    # topicãŒãªã„ã‚°ãƒ«ãƒ¼ãƒ—ã¯é™¤ã
                    sim_texts = [sim_text for sim_text in sim_texts if sim_text[0] != '']
                    sim_texts = sim_texts[:20] # top20
                    
                    # 20ã«æº€ãŸãªã„å ´åˆã¯Noneã§åŸ‹ã‚ã‚‹
                    sim_length = len(sim_texts)

                    for k in range(20 - sim_length):
                        sim_texts.append(None)
                
                sim_texts_all += sim_texts
                
                
            '''
            # tweetdata2ã€€ãƒ†ãƒ¼ãƒ–ãƒ«æ›´æ–°
            all_ids = [obj['t_id'] for obj in allobj]
            
            summary_no_updates = []
            for i in all_ids:
                breakflag = False
                for k,sim_text in enumerate(sim_texts_all):
                    for sim in sim_text[1:]: # 0ç•ªç›®ã¯è¦ç´„ã®ã¿,1ç•ªç›®ä»¥é™ã«(tweet,id)ï½
                        if i == sim[1]:
                            summary_no_updates.append(k + 1)
                            breakflag = True
                            break
                    if breakflag == True:
                        break
                if breakflag == False:
                    summary_no_updates.append(0)
                    
    
            update_list = []
            for summary_no_update,obj in zip(summary_no_updates,data):
               
                obj.summary_no = summary_no_update
                update_list.append(obj)
    
            print('bulk update..')
            # ã‚³ãƒ”ãƒ¼ã‚’å–å¾—ã—ãŸæ™‚ã¨åŒã˜é †åºã§ãªã„ã¨æ­£ã—ã„é †ç•ªã§bulk updateã§ããªã„
            data.bulk_update(update_list,
                             fields=['summary_no',
                                     ],
                             batch_size=5000
                             )
            
            best_summarys = [sim_text[0] for sim_text in sim_texts]
            
            '''    

            # tweetdata2ã€€ãƒ†ãƒ¼ãƒ–ãƒ«æ›´æ–°(bulk updateãªã—)
            best_summarys = []
            for i,sim_text in enumerate(sim_texts_all):
                if sim_text == None:
                    best_summarys.append(None)
                    continue
                else:
                    best_summarys.append(sim_text.pop(0))
                # summary_no æ›´æ–°
                for sim in sim_text:
                    obj = Tweetdata2.objects.get(t_id = sim[1])
                    obj.summary_brand_id = target_no
                    obj.summary_no = i + 1
                    obj.save()
                    

                    
            # Summary ãƒ†ãƒ¼ãƒ–ãƒ«æ›´æ–°
            print("Summarys update..")
    
            obj, created = Summarys.objects.update_or_create(
                        official_name_id = target_no,
                        defaults={
                        'summary_text1' : best_summarys[0],
                        'summary_text2' : best_summarys[1],
                        'summary_text3' : best_summarys[2],
                        'summary_text4' : best_summarys[3],
                        'summary_text5' : best_summarys[4],            
                        'summary_text6' : best_summarys[5],
                        'summary_text7' : best_summarys[6],
                        'summary_text8' : best_summarys[7],
                        'summary_text9' : best_summarys[8],
                        'summary_text10' : best_summarys[9],
                        'summary_text11' : best_summarys[10],
                        'summary_text12' : best_summarys[11],
                        'summary_text13' : best_summarys[12],
                        'summary_text14' : best_summarys[13],
                        'summary_text15' : best_summarys[14],
                        'summary_text16' : best_summarys[15],
                        'summary_text17' : best_summarys[16],
                        'summary_text18' : best_summarys[17],
                        'summary_text19' : best_summarys[18],
                        'summary_text20' : best_summarys[19],
                        #ä»¥ä¸‹nega
                        'summary_text21' : best_summarys[20],
                        'summary_text22' : best_summarys[21],
                        'summary_text23' : best_summarys[22],
                        'summary_text24' : best_summarys[23],
                        'summary_text25' : best_summarys[24],            
                        'summary_text26' : best_summarys[25],
                        'summary_text27' : best_summarys[26],
                        'summary_text28' : best_summarys[27],
                        'summary_text29' : best_summarys[28],
                        'summary_text30' : best_summarys[29],
                        'summary_text31' : best_summarys[30],
                        'summary_text32' : best_summarys[31],
                        'summary_text33' : best_summarys[32],
                        'summary_text34' : best_summarys[33],
                        'summary_text35' : best_summarys[34],
                        'summary_text36' : best_summarys[35],
                        'summary_text37' : best_summarys[36],
                        'summary_text38' : best_summarys[37],
                        'summary_text39' : best_summarys[38],
                        'summary_text40' : best_summarys[39],
                        }
                        )
            if created == True:
                print("added new record.")
            else:
                print("updated record.")
                
            print('all done:{0}'.format(target_no))        
      
        
      
        
      
        
        '''
    # è¦ç´„å–å¾—
        print("take summarys..")
        documents = [(obj['content'],obj['t_id']) for obj in allobj if obj['s_class'] == 4]
        documents = [(cleaning(d[0]),d[1]) for d in documents] # å‰å‡¦ç†
        documents = [d for d in documents if len(d[0])<=80]
        documents = get_unique_list(documents,0)
        
        if documents == None:
            print('No p2 data.')
            continue
        
        numd = len(documents)
        print('len(p2):',numd)

        if len(documents) == 0:
            print('No p2 data.')
            continue
        else:
            
            if numd > 1000:
                random.shuffle(documents)
                documents = documents[:500]
            elif numd < 100:
                # p2å°‘ãªã„å ´åˆp1ã‚‚åˆã‚ã›ã‚‹
                documents_p1 = [(obj['content'],obj['t_id']) for obj in allobj if obj['s_class'] == 3]
                #documents_p1 = data.filter(s_class=3).values_list('content', 't_id') # ã‚¿ãƒ—ãƒ«ã§è¿”å´ã•ã‚Œã‚‹

                documents_p1 = [(cleaning(d[0]),d[1]) for d in documents_p1] # å‰å‡¦ç†
                documents_p1 = [d for d in documents_p1 if len(d[0])<=80]
                documents_p1 = get_unique_list(documents_p1,0)
                if documents_p1 == None:
                    return 'No p1&p2 data.'
                print('len(p1):',len(documents_p1))
                documents += documents_p1
                random.shuffle(documents)                
                documents = documents[:500]
                numd = len(documents)
                if numd == 0:
                    return 'No p1&p2 data.'
        
            num_topics = round(math.log(numd,5))
            tmp_summarys = []
            for i in range(1): # ä»®ã§1å›.æœ€çµ‚çš„ã«ã¯3å›å®Ÿæ–½ã«å¤‰æ›´
                print('{0}å›ç›®å–å¾—é–‹å§‹'.format(i+1))
                result = take_summary(keywords,documents,maxlength=50,num_topics=num_topics,num_summary=5)
                if len(result) > 0:
                    tmp_summarys.append(result)
           
            summarys = [s for s in tmp_summarys if len(s) >= 3] # è¦ç´„æ•°ã¯æœ€ä½3ã¤

            if len(summarys) == 0: # æœ€ä½ï¼“ã¤ç„¡ã‘ã‚Œã°è¦ç´„æ•°ã®æ¡ä»¶ãªã—
                summarys = tmp_summarys

            # æœ€å¤§æŠ•ç¨¿æ•°ã‚’æŒã¤è¦ç´„ã‚’é¸ã¶    
            max_posts = 0
            best_summarys = []
            for summary in summarys:
                num_posts = sum([s['æŠ•ç¨¿æ•°'] for s in summary])
                if num_posts > max_posts:
                    best_summarys = summary
                    max_posts = num_posts
        print('bestsummarys:')
        pprint.pprint(best_summarys)
        print('max_posts:',max_posts)
    
        # t_idå–å¾—    
        t_ids = [obj['t_id'] for obj in allobj] 
        #t_ids = list(data.values_list("t_id", flat=True))
        
        summary_no_updates =[]
        #summary_text_updates = []
        for t_id in t_ids:
            breakflag = False
            for k,best_summary in enumerate(best_summarys):
                for org in best_summary['åŸæ–‡']:
                    if t_id == org[1]:
                        summary_no_updates.append(k+1)
                        #summary_text_updates.append(best_summary['è¦ç´„'])
                        breakflag = True
                        break
                if breakflag == True:
                    break
            if breakflag == False: # è¦ç´„ã§ã¯ãªã„æŠ•ç¨¿ã®å ´åˆ
                summary_no_updates.append(0)
                #summary_text_updates.append(None)            
                
        if len([s for s in summary_no_updates if s > 0]) != max_posts:
            return 'ERROR!(id numbers:{0} does not match with summary numbers)'.format(len([s for s in summary_no_updates if s > 0]))
        if len(data) != len(summary_no_updates):
            return 'ERROR!(summary_update != data)'
        
        print("DB update..")
        update_list = []
        print("making update_list..")
        
        for summary_no_update,obj in zip(summary_no_updates,data):
           
            obj.summary_no = summary_no_update
            update_list.append(obj)
    
        print('done.')
        print('bulk update..')
        # ã‚³ãƒ”ãƒ¼ã‚’å–å¾—ã—ãŸæ™‚ã¨åŒã˜é †åºã§ãªã„ã¨æ­£ã—ã„é †ç•ªã§bulk updateã§ããªã„
        
        data.bulk_update(update_list,
                         fields=['summary_no',
                                 ],
                         batch_size=5000
                         )
        
        print('done.')
        
        # Summaryæ›´æ–°
        print("Summarys update..")

        obj, created = Summarys.objects.update_or_create(
                    official_name_id = title_no,
                    defaults={
                    'summary_text1' : best_summarys[0]['è¦ç´„'] if len(best_summarys) >= 1 else None,
                    'summary_text2' : best_summarys[1]['è¦ç´„'] if len(best_summarys) >= 2 else None,
                    'summary_text3' : best_summarys[2]['è¦ç´„'] if len(best_summarys) >= 3 else None,
                    'summary_text4' : best_summarys[3]['è¦ç´„'] if len(best_summarys) >= 4 else None,
                    'summary_text5' : best_summarys[4]['è¦ç´„'] if len(best_summarys) >= 5 else None,
                    'summary_text6' : best_summarys[5]['è¦ç´„'] if len(best_summarys) >= 6 else None,
                    'summary_text7' : best_summarys[6]['è¦ç´„'] if len(best_summarys) >= 7 else None,
                    'summary_text8' : best_summarys[7]['è¦ç´„'] if len(best_summarys) >= 8 else None,
                    'summary_text9' : best_summarys[8]['è¦ç´„'] if len(best_summarys) >= 9 else None,
                    'summary_text10' : best_summarys[9]['è¦ç´„'] if len(best_summarys) >= 10 else None,
                    'summary_text11' : best_summarys[10]['è¦ç´„'] if len(best_summarys) >= 11 else None,
                    'summary_text12' : best_summarys[11]['è¦ç´„'] if len(best_summarys) >= 12 else None,
                    'summary_text13' : best_summarys[12]['è¦ç´„'] if len(best_summarys) >= 13 else None,
                    'summary_text14' : best_summarys[13]['è¦ç´„'] if len(best_summarys) >= 14 else None,
                    'summary_text15' : best_summarys[14]['è¦ç´„'] if len(best_summarys) >= 15 else None,
                    'summary_text16' : best_summarys[15]['è¦ç´„'] if len(best_summarys) >= 16 else None,
                    'summary_text17' : best_summarys[16]['è¦ç´„'] if len(best_summarys) >= 17 else None,
                    'summary_text18' : best_summarys[17]['è¦ç´„'] if len(best_summarys) >= 18 else None,
                    'summary_text19' : best_summarys[18]['è¦ç´„'] if len(best_summarys) >= 19 else None,
                    'summary_text20' : best_summarys[19]['è¦ç´„'] if len(best_summarys) >= 20 else None,
                    }
                    )
        if created == True:
            print("added new record.")
        else:
            print("updated record.")
                
        print('all done:{0}'.format(title_no))
        '''
        
 
        
    return "done."    


def clean_trend():
    ### Tweetdata2ã‹ã‚‰trendã‚¯ãƒªã‚¢ ###
    
    
    dt_now = datetime.datetime.now()
    e_date = datetime.datetime(dt_now.year,
                              dt_now.month, 
                              dt_now.day, 0, 0, 0, 0,tzinfo=datetime.timezone.utc) - datetime.timedelta(hours=9)
    s_date = e_date - datetime.timedelta(days=7)
    
    data = Tweetdata2.objects.filter(Q(spam=False),
                                     Q(t_date__range=[s_date,e_date]),
                                       trend_no__gte=1,
                                       ).only('id','trend_no').order_by('id') #
    print('len(data):',len(data))
    update_list = []
    for obj in data:
       
        obj.trend_no = 0
        update_list.append(obj)
        
    print("deleting tweetdata2 trends..")
    data.bulk_update(update_list,
                    fields=['trend_no',
                            ],
                    batch_size=5000
                    ) 
    print("done.")
    

                                                           
    return 'done.'


def trend_update():

    dt_now = datetime.datetime.now()
    e_date = datetime.datetime(dt_now.year,
                              dt_now.month, 
                              dt_now.day, 0, 0, 0, 0,tzinfo=datetime.timezone.utc) - datetime.timedelta(hours=9)
    s_date = e_date - datetime.timedelta(days=7)
    print('date:{0}-{1}'.format(s_date,e_date))  

    # å…¨ã‚­ãƒ£ãƒ©åå–å¾—
    keywords = list(Keywords.objects.all().values_list('keyword', flat=True))
    print('keywords:',keywords)
    print("done.")
    
    title_nos = list(Official_names.objects.filter(title=True,index__gte=1).values_list('index',flat=True))
    chara_nos = list(Official_names.objects.filter(title=False,index__gte=1).values_list('index',flat=True))
    print('titles:',title_nos)
    print('charas',chara_nos)


    for cnt in range(2):
        if cnt == 0:
            target_nos = title_nos
        else:
            target_nos = chara_nos
                
        for target_no in target_nos:
            print("accesing DB:{0}".format(target_no))
                #DBã‹ã‚‰æœ¬æ–‡å–å¾—
            if cnt == 0:
                data = Tweetdata2.objects.filter(Q(spam=False),
                                                 Q(trend_no=0),
                                                 Q(s_class=0) | Q(s_class=1) | Q(s_class=3) | Q(s_class=4),
                                                 Q(t_date__range=[s_date,e_date]),
                                                 Q(title1=target_no) | Q(title2=target_no) | Q(title3=target_no),
                                                   ).only('id',
                                                          't_id',
                                                          'hashtag',
                                                          'trend_no',
                                                          ).order_by('id') # ä½•ã‚‰ã‹ã®ã‚­ãƒ¼ã§ä¸¦ã³æ›¿ãˆãªã„ã¨bulk updateãŒæ­£ã—ã„é †åºã§updateã§ããªã„ã‚‰ã—ã„
            else:
                data = Tweetdata2.objects.filter(Q(spam=False),
                                                 Q(trend_no=0),
                                                 Q(s_class=0) | Q(s_class=1) | Q(s_class=3) | Q(s_class=4),
                                                 Q(t_date__range=[s_date,e_date]),
                                                 Q(character1=target_no) | Q(character2=target_no) | Q(character3=target_no) | Q(character4=target_no) | Q(character5=target_no),
                                                   ).only('id',
                                                          't_id',
                                                          'hashtag',
                                                          'trend_no',
                                                          ).order_by('id') # ä½•ã‚‰ã‹ã®ã‚­ãƒ¼ã§ä¸¦ã³æ›¿ãˆãªã„ã¨bulk updateãŒæ­£ã—ã„é †åºã§updateã§ããªã„ã‚‰ã—ã„
        
                                                          
                                                      
            allobj = list(data.values())                                                  
            print("done. len:{0}".format(len(allobj)))
        
            # trendå–å¾—
            print("take trends..")
    
            # ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°å–å¾—    
            hashtags = [(obj['t_id'],obj['hashtag']) for obj in allobj]
            # hashtags=[(id,'[a]'),(id2,'[a,b]'),...] å„è¦ç´ ã¯strãªã®ã§ãƒªã‚¹ãƒˆã«æˆ»ã™
            
            hashtags_list=[]
    
            for h in hashtags:
            
                if h[1] != str([]): # åˆæœŸãƒ‡ãƒ¼ã‚¿ã¯ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚° None
                                         ## ç©ºæ¬„ã¨"'"ã¯é™¤å¤–(ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°é€”ä¸­ã«å‡ºç¾ã—ãŸå ´åˆï¼Ÿ)
                    li = h[1].replace("'","").replace(" ","")[1:-1].split(",")
                    li = [l for l in li if len(l) <= 30] # modelã¯30å­—æœªæº€
                    
                    hashtags_list.append((h[0],li))
    
                else:
                    continue
    
            # hashtagsã‚’å±•é–‹ã—ã¦1æ¬¡å…ƒãƒªã‚¹ãƒˆåŒ–ã€è¦ç´ ã‚«ã‚¦ãƒ³ãƒˆ        
            c = collections.Counter(list(itertools.chain.from_iterable([h[1] for h in hashtags_list])))
            trend_words=[]
    
            # ä¸€æ—¦top10ã®ã¿è¡¨ç¤º
            ## ãƒˆãƒ¬ãƒ³ãƒ‰10å€‹æœªæº€ã®å ´åˆ
            if len(c)<10:
                pass
            else:
                for i in range(10): 
                    # top10ã«è©²å½“ã™ã‚‹t_idå–å¾—
                    update_ids = []
                    for h in hashtags_list:
                        if c.most_common()[i][0] in h[1]:
                            update_ids.append(h[0])
                            
                    trend_words.append((c.most_common()[i][0],update_ids))
            '''
            ##ç¾çŠ¶tweetdataã®trend_noã¯æœªä½¿ç”¨ã€‚updateã¯ãªã—
            
            # tweetdata2ã€€ãƒ†ãƒ¼ãƒ–ãƒ«æ›´æ–°
            all_ids = [obj['t_id'] for obj in allobj]
            
            trend_no_updates = []
            for i in all_ids:
                breakflag = False
                for k,trend_word in enumerate(trend_words):
                    if i in trend_word[1]:
                        trend_no_updates.append(k + 1)
                        breakflag = True
                        break
                if breakflag == False:
                    trend_no_updates.append(0)
    
            update_list = []
            for trend_no_update,obj in zip(trend_no_updates,data):
               
                obj.trend_no = trend_no_update
                update_list.append(obj)
        
            print('bulk update..')
            # ã‚³ãƒ”ãƒ¼ã‚’å–å¾—ã—ãŸæ™‚ã¨åŒã˜é †åºã§ãªã„ã¨æ­£ã—ã„é †ç•ªã§bulk updateã§ããªã„
            data.bulk_update(update_list,
                             fields=['trend_no',
                                     ],
                             batch_size=5000
                             )
            
            #print("trendwords",trend_words)                    
            '''
            # Summary ãƒ†ãƒ¼ãƒ–ãƒ«æ›´æ–°
            print("Trends update..")
    
            obj, created = Trends.objects.update_or_create(
                        official_name_id = target_no,
                        defaults={
                        'trend1' : trend_words[0][0] if len(trend_words) >= 1 else None,
                        'trend1_count' : len(trend_words[0][1]) if len(trend_words) >= 1 else None,
                        'trend2' : trend_words[1][0] if len(trend_words) >= 2 else None,
                        'trend2_count' : len(trend_words[1][1]) if len(trend_words) >= 2 else None,
                        'trend3' : trend_words[2][0] if len(trend_words) >= 3 else None,
                        'trend3_count' : len(trend_words[2][1]) if len(trend_words) >= 3 else None,
                        'trend4' : trend_words[3][0] if len(trend_words) >= 4 else None,
                        'trend4_count' : len(trend_words[3][1]) if len(trend_words) >= 4 else None,
                        'trend5' : trend_words[4][0] if len(trend_words) >= 5 else None,
                        'trend5_count' : len(trend_words[4][1]) if len(trend_words) >= 5 else None,
                        'trend6' : trend_words[5][0] if len(trend_words) >= 6 else None,
                        'trend6_count' : len(trend_words[5][1]) if len(trend_words) >= 6 else None,
                        'trend7' : trend_words[6][0] if len(trend_words) >= 7 else None,
                        'trend7_count' : len(trend_words[6][1]) if len(trend_words) >= 7 else None,
                        'trend8' : trend_words[7][0] if len(trend_words) >= 8 else None,
                        'trend8_count' : len(trend_words[7][1]) if len(trend_words) >= 8 else None,
                        'trend9' : trend_words[8][0] if len(trend_words) >= 9 else None,
                        'trend9_count' : len(trend_words[8][1]) if len(trend_words) >= 9 else None,
                        'trend10' : trend_words[9][0] if len(trend_words) >= 10 else None,
                        'trend10_count' : len(trend_words[9][1]) if len(trend_words) >= 10 else None,
                        }
                        )
            if created == True:
                print("added new record.")
            else:
                print("updated record.")
                
            print('all done:{0}'.format(target_no))        
      
    


def make_wordcloud():

    
    dt_now = datetime.datetime.now()
    e_date = datetime.datetime(dt_now.year,
                              dt_now.month, 
                              dt_now.day, 0, 0, 0, 0,tzinfo=datetime.timezone.utc) - datetime.timedelta(hours=9)
    s_date = e_date - datetime.timedelta(days=7)
    print('date:{0}-{1}'.format(s_date,e_date))  
    
    title_nos = list(Official_names.objects.filter(title=True,index__gte=1).values_list('index','official_name'))
    chara_nos = list(Official_names.objects.filter(title=False,index__gte=1).values_list('index','official_name'))
    print('titles:',title_nos)
    print('charas',chara_nos) 

    
    for cnt in range(2):
        if cnt == 0:
            target_nos = title_nos
        else:
            target_nos = chara_nos
                
        for target_no in target_nos:
            
            print("making wordcloud:{0}".format(target_no[1]))
                #DBã‹ã‚‰æœ¬æ–‡å–å¾—
            
            if cnt == 0:
                data = Tweetdata2.objects.filter(Q(spam=False),
                                                 Q(t_date__range=[s_date,e_date]),
                                                 Q(s_class=0) | Q(s_class=1) | Q(s_class=3) | Q(s_class=4),
                                                 Q(title1=target_no[0]) | Q(title2=target_no[0]) | Q(title3=target_no[0]),
                                                   ).only('id',
                                                          's_class',
                                                          'wakachi'
                                                          ).order_by('id') # ä½•ã‚‰ã‹ã®ã‚­ãƒ¼ã§ä¸¦ã³æ›¿ãˆãªã„ã¨bulk updateãŒæ­£ã—ã„é †åºã§updateã§ããªã„ã‚‰ã—ã„
            else:
                data = Tweetdata2.objects.filter(Q(spam=False),
                                                 Q(t_date__range=[s_date,e_date]),
                                                 Q(s_class=0) | Q(s_class=1) | Q(s_class=3) | Q(s_class=4),
                                                 Q(character1=target_no[0]) | Q(character2=target_no[0]) | Q(character3=target_no[0]) | Q(character4=target_no[0]) | Q(character5=target_no[0]),
                                                   ).only('id',
                                                          's_class',
                                                          'wakachi'
                                                          ).order_by('id') # ä½•ã‚‰ã‹ã®ã‚­ãƒ¼ã§ä¸¦ã³æ›¿ãˆãªã„ã¨bulk updateãŒæ­£ã—ã„é †åºã§updateã§ããªã„ã‚‰ã—ã„
    
                                                      
            allobj = list(data.values())                                                  
            print("done. len:{0}".format(len(allobj)))
        # wordcloud
            print("making wordcloud..")
            wakachi = [obj['wakachi'] for obj in allobj if obj['wakachi'] != None]
        
            if len(wakachi) < 50:
                print("æŠ•ç¨¿ä¸è¶³")
                continue
                
            else:
                # ãƒã‚¹ã‚¯ã‚’ä½œæˆã™ã‚‹
                #path = r'static\sentiment\image\wordcloud\{0}.png'.format(anime)
                if cnt == 0:
                    path = os.path.join(BASE_DIR, 'static\image\wordcloud\org_anime\{0}.png'.format(target_no[1]))
                else:
                    path = os.path.join(BASE_DIR, 'static\image\wordcloud\org_character\{0}.png'.format(target_no[1]))
    
                try:
                    mask_array = np.array(Image.open(os.path.join(os.path.dirname(os.path.abspath(__file__)),path)))
                except FileNotFoundError as e:
                    print(e)
                    mask_array = None
                # â†“ã¾ã¨ã¾ã£ãŸã‚‰labelsaveã«ç§»å‹•
                adj_stopwords = ['å¤šã„','å°‘ãªã„','ã¾ã˜','ãƒã‚¸','å¯èƒ½','å¤§å¤‰','ã‚ˆã‚ã—ã„','å®œã—ã„','æ°—è»½','å¹¸ã„','å¤±ç¤¼','ã½ã„','ã†ã„','æ¬²ã—ã„','ã‚„ã™ã„','ã¥ã‚‰ã„','ã«ãã„','ãªã„','ç„¡ã„','ã„ã„','è‰¯ã„','ã‚ˆã„','ã‚„ã°ã„','ã£ã½ã„'] 
                noun_stopwords = ['ã‚¢ãƒãƒ—ãƒ©','ç‰¹å…¸','ã‚¬ãƒ³','æ¬¡','å›','å¾¡ä¼½','ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³','å…¨å·»','ã‚¢ãƒ‹ãƒ¡','æ¼«ç”»','å˜å“','æœˆé¡','æ„Ÿæƒ³','å††','å‡ºã—æ‰‹','æ„Ÿã˜','è‡ªåˆ†','ç§','åƒ•','ã‚³ãƒŸãƒƒã‚¯','ã‚­ãƒ£ãƒ©','å­','äºº','å®šæœŸ','ä½œå“','ãƒ©ãƒ³ã‚­ãƒ³ã‚°','ãƒãƒˆãƒ•ãƒª','ãªã˜ã¿','æ°—','ã‚ã¨']
                #wordcloud = WordCloud(mask=mask_array, background_color='white', colormap='bone', contour_width=3, contour_color='gray')        
                wordcloud = WordCloud(colormap='brg',
                                      width=400,
                                      height=400,
                                      min_font_size=9,
                                      max_words=50,
            
                                      contour_width=0.001,
                                      contour_color='powderblue',
                                      font_path=r"C:/Users/yusuk/AppData/Local/Microsoft/Windows/Fonts/NotoSansJP-Light.otf",
                                      mask=mask_array,
                                      background_color='white',
                                      stopwords=adj_stopwords + noun_stopwords,
                                      collocations=False)
                
                wordcloud = wordcloud.generate(' '.join(wakachi))
                # fileä¿å­˜
                #ftime = re.sub("[- :\.]","_",str(datetime.datetime.now()))
                fname = '{0}_wc'.format(target_no[1])
                fname = r'{0}.png'.format(fname)
                #print('fname:',fname)
                #print('aa:',r'sentiment\image\wordcloud\anime\png\{0}'.format(fname))
                if cnt == 0:
                    wordcloud = wordcloud.to_file(os.path.join(BASE_DIR, r'static\image\wordcloud\result\anime\png\{0}'.format(fname)))
                else:
                    wordcloud = wordcloud.to_file(os.path.join(BASE_DIR, r'static\image\wordcloud\result\character\png\{0}'.format(fname)))
                
                '''
                # PILã§è¡¨ç¤ºã™ã‚‹
                image_array = wordcloud.to_array()
                img = Image.fromarray(image_array)
                
                buf = io.BytesIO()
                img.save(buf,format='png')
                s = buf.getvalue()
                s = base64.b64encode(s).decode()
                buf.close()
                plt.cla()
                fname = s
                '''

    return "success."  


PREF = [re.compile('åŒ—æµ·é“|æœ­å¹Œ|hokkaido|sapporo'),re.compile('é’æ£®|aomori'),re.compile('å²©æ‰‹|ç››å²¡|iwate|morioka'),re.compile('å®®åŸ|ä»™å°|miyagi|sendai'),re.compile('ç§‹ç”°|akita'),re.compile('å±±å½¢|yamagata'),re.compile('ç¦å³¶|fukushima'),re.compile('èŒ¨åŸ|æ°´æˆ¸|ibaraki|mito'),re.compile('æ ƒæœ¨|å®‡éƒ½å®®|tochigi|utunomiya'),re.compile('ç¾¤é¦¬|å‰æ©‹|gunma|maebashi'),re.compile('åŸ¼ç‰|ã•ã„ãŸã¾|saitama'),re.compile('åƒè‘‰|chiba'),re.compile('æ±äº¬|tokyo|æ–°å®¿|æ¸‹è°·|åƒä»£ç”°åŒº|ä¸–ç”°è°·åŒº'),re.compile('ç¥å¥ˆå·|æ¨ªæµœ|kanagawa|yokohama'),re.compile('æ–°æ½Ÿ|niigata'),re.compile('å¯Œå±±|toyama'),re.compile('çŸ³å·|é‡‘æ²¢|ishikawa|kanazawa'),re.compile('ç¦äº•|fukui'),re.compile('å±±æ¢¨|ç”²åºœ|yamanashi|kofu'),re.compile('é•·é‡|nagano'),re.compile('å²é˜œ|gihu'),re.compile('é™å²¡|shizuoka'),re.compile('æ„›çŸ¥|åå¤å±‹|aichi|nagoya'),re.compile('ä¸‰é‡|mie|tsu'),re.compile('æ»‹è³€|å¤§æ´¥|shiga|otsu'),re.compile('äº¬éƒ½|kyoto'),re.compile('å¤§é˜ª|osaka'),re.compile('å…µåº«|ç¥æˆ¸|hyogo'),re.compile('å¥ˆè‰¯|nara'),re.compile('å’Œæ­Œå±±|wakayama'),re.compile('é³¥å–|tottori'),re.compile('å³¶æ ¹|æ¾æ±Ÿ|shimane|matsue'),re.compile('å²¡å±±|okayama'),re.compile('åºƒå³¶|hiroshima'),re.compile('å±±å£|yamaguchi'),re.compile('å¾³å³¶|tokushima'),re.compile('é¦™å·|é«˜æ¾|kagawa|takamatsu'),re.compile('æ„›åª›|æ¾å±±|ehime|matsuyama'),re.compile('é«˜çŸ¥|kouchi'),re.compile('ç¦å²¡|fukuoka'),re.compile('ä½è³€|saga'),re.compile('é•·å´|nagasaki'),re.compile('ç†Šæœ¬|kumamoto'),re.compile('å¤§åˆ†|oita'),re.compile('å®®å´|miyazaki'),re.compile('é¹¿å…å³¶|kagoshima'),re.compile('æ²–ç¸„|é‚£è¦‡|okinawa|naha'),re.compile('é–¢æ±|kanto'),re.compile('é–¢è¥¿|kansai'),re.compile('æ±åŒ—|tohoku'),re.compile('å››å›½|shokoku'),re.compile('ä¹å·|kyushu'),re.compile('æ—¥æœ¬|japan'),re.compile('ä¸–ç•Œ|world'),re.compile('åœ°çƒ|earth'),re.compile('å®‡å®™|universe')]               
        
def prefectures_update():
    
    print("accessing the spreadsheet..",end='')
    gc = gspread.service_account()
    #ãƒ•ã‚¡ã‚¤ãƒ«ã‚ªãƒ¼ãƒ—ãƒ³
    sh = gc.open_by_key('1b-89Ynrpek39vny4sPfkaYqNp-WYLZJ6txAjaEAld9I') # anime_keywords
    worksheet = sh.worksheet('pref')

    
    # pref
    indexes = [i for i in worksheet.col_values(1)]
    indexes.pop(0)
    prefectures = [i for i in worksheet.col_values(2)]
    prefectures.pop(0)    
    
    print(prefectures)
  
    print("deleting DB..")
    
    Prefectures.objects.all().delete()
    
    print("done.")
    
    print('bulk update..')                                              
    update_list = []
    for index,pref in zip(indexes,prefectures):
        update_list.append(Prefectures(prefecture = pref,
                                       id = index,
                                          ))

        
    Prefectures.objects.bulk_create(update_list)


    print("done.")


    return 'success.'
    
def set_prefectures():

    dt_now = datetime.datetime.now()
    e_date = datetime.datetime(dt_now.year,
                              dt_now.month, 
                              dt_now.day, 0, 0, 0, 0,tzinfo=datetime.timezone.utc) - datetime.timedelta(hours=9)
    s_date = e_date - datetime.timedelta(days=7)
    print('date:{0}-{1}'.format(s_date,e_date))  



    
    data = Tweetdata2.objects.filter(Q(spam=False),
                                     Q(t_date__range=[s_date,e_date]),
                                     # title1ã®ã¿ï¼ˆä»–ã‚‚è¦‹ã‚‹ã¨è¤‡æ•°ã‚¢ãƒ‹ãƒ¡ã®è¦ç´„ãŒä¸€ç·’ã«ãªã£ã¦ã—ã¾ã†
                                       ).only('id',
                                              'location',
                                              'prefecture'
                                              ).order_by('id') # ä½•ã‚‰ã‹ã®ã‚­ãƒ¼ã§ä¸¦ã³æ›¿ãˆãªã„ã¨bulk updateãŒæ­£ã—ã„é †åºã§updateã§ããªã„ã‚‰ã—ã„

                                              
    allobj = list(data.values())                                                  
    print("done. len:{0}".format(len(allobj)))

# prefectureå–å¾—
    locations =  [obj['location'] for obj in allobj if obj['location'] != None]    
    print(locations[:100])
    print("getting prefs..")
    pref_ids=[]
    for loc in locations:
        if loc in ["",None]:
            pref_ids.append(None)
            continue
        loc = loc.lower()
        for k,p in enumerate(PREF):
            if p.search(loc):
                pref_ids.append(k+1)
                break
            if k + 1 == len(PREF):
                # ä½•ã‚‚è©²å½“ã—ãªã„å ´åˆ
                pref_ids.append(None)
        
    print('done.')
    
    print('bulk update..')
    update_list = []
    for pref_id, obj in zip(pref_ids,data):
        obj.prefecture = pref_id
        update_list.append(obj)

    data.bulk_update(update_list,
                     fields=['prefecture'
                             ],
                     batch_size=5000
                     )        
    print('done.')
    return 'success.'


if __name__=='__main__':

    days = int(input("days:"))
    '''
    # ä¸€è¦§è¡¨æ›´æ–°
    msg = official_names_update()
    print(msg)
    msg = keywords_update()
    print(msg)
    
    
    
    # ãƒ©ãƒ™ãƒ«ã€ã‚¹ãƒ‘ãƒ ,title,chara,å½¢æ…‹ç´ æ›´æ–°
    msg=dataupdate(days) 
    print(msg)
    
    # 1é€±é–“çµ±è¨ˆä½œæˆ(è¦ç´„ä»¥å¤–)
    msg=statistics_update()
    print(msg)
    '''
    #è¦ç´„ä½œæˆ
    msg=clean_summary()
    print(msg)
    
    msg=sumupdate()
    print(msg)
    
    #trendä½œæˆ
    msg=clean_trend()
    print(msg)
    
    msg=trend_update()
    print(msg)
    
    # wordcloudã‚¤ãƒ¡ãƒ¼ã‚¸ä½œæˆ
    msg=make_wordcloud()
    print(msg)
    
    
    #msg=prefectures_update()
    #print(msg)
    
    
    msg=set_prefectures()
    print(msg)
    
